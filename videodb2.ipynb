{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8be64453",
      "metadata": {},
      "source": [
        "# VideoDB for STS3E4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "940295ce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: videodb in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (0.3.0)\n",
            "Requirement already satisfied: openai in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (2.17.0)\n",
            "Requirement already satisfied: python-dotenv in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (1.2.1)\n",
            "Requirement already satisfied: requests>=2.25.1 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from videodb) (2.32.4)\n",
            "Requirement already satisfied: backoff>=2.2.1 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from videodb) (2.2.1)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from videodb) (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from openai) (0.13.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from requests>=2.25.1->videodb) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/utkarsh/Desktop/github/MLOps-base/demo_dagshub/venv/lib/python3.10/site-packages (from requests>=2.25.1->videodb) (2.5.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install SDK (run once)\n",
        "%pip install videodb openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e25ddf60",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'m-z-019c3af7-6cfc-7e82-9128-ffc5e515e796'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import videodb\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from videodb import SceneExtractionType\n",
        "\n",
        "base_dir = os.getcwd()\n",
        "dotenv_path = os.path.join(base_dir, \".env\")\n",
        "load_dotenv(dotenv_path=dotenv_path, override=True)\n",
        "\n",
        "videodb_api_key = os.getenv(\"VIDEO_DB_API_KEY\")\n",
        "if not videodb_api_key:\n",
        "    raise ValueError(\n",
        "        f\"Missing VIDEO_DB_API_KEY. Looked in {dotenv_path} (exists={os.path.exists(dotenv_path)})\"\n",
        "    )\n",
        "\n",
        "conn = videodb.connect(api_key=videodb_api_key)\n",
        "\n",
        "video_path = \"./videos/STS3E4.mp4\" \n",
        "video = conn.upload(file_path=video_path)\n",
        "\n",
        "video.id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "97db4b6a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcript text preview:\n",
            "Sam. Nancy. Nancy. You're so beautiful. Sa. Steve. Hey, Steve. I'll see you tomorrow, okay? It. Oh, Jesus. You scared me. I scared you? I know. I should have called. Where have you been? We agreed on Zen after the assembly. Some people wanted to get something to eat. I didn't think it'd be a big deal. You didn't think to call and let me know? With everything that's been going on, I didn't realize how late it was, okay? I'm sorry, Mom. What more do you want? Hey, wait. Whose sweatshirt is that? Steve's. Steve's? So is Steve your boyfriend now? What? No, it was just cold, so I borrowed his sweatshirt. It's no big deal. Nancy. What? You can talk to me. You can talk to me. Whatever happened. Nothing happened, Nancy. Nothing happened. Can I please go? Please? Me? Please. Just talk to me. Talk to me. Just say. Mom. Jonathan, Come here. What is this? Come here. Come here. What's going on? It's Will. It's Will. He's trying to talk to me. He's trying to talk to you? Yes. Through the lights. Mom\n"
          ]
        }
      ],
      "source": [
        "# Transcript generation\n",
        "video.generate_transcript()\n",
        "\n",
        "transcript = video.get_transcript()  # timestamped segments\n",
        "transcript_text = video.get_transcript_text()\n",
        "\n",
        "print(\"Transcript text preview:\")\n",
        "print(transcript_text[:1000])\n",
        "\n",
        "# Save raw transcript for later processing\n",
        "os.makedirs(\"./outputs\", exist_ok=True)\n",
        "with open(\"./outputs/sts3e4_transcript.json\", \"w\") as f:\n",
        "    json.dump(transcript, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2c99f6ed",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['video_id', 'video_path', 'transcript', 'scenes'])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Scene understanding (visual indexing)\n",
        "scene_index_id = video.index_scenes(\n",
        "    extraction_type=SceneExtractionType.shot_based,\n",
        "    prompt=\"Describe the scene with a focus on products, brands, packaging, and any visible labels.\"\n",
        ")\n",
        "\n",
        "scenes = video.get_scene_index(scene_index_id)\n",
        "\n",
        "# Build structured payload for downstream GPT\n",
        "payload = {\n",
        "    \"video_id\": video.id,\n",
        "    \"video_path\": video_path,\n",
        "    \"transcript\": transcript,\n",
        "    \"scenes\": scenes,\n",
        "}\n",
        "\n",
        "with open(\"./outputs/sts3e4_scene_and_transcript.json\", \"w\") as f:\n",
        "    json.dump(payload, f, indent=2)\n",
        "\n",
        "payload.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dc6f202c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1/20 | scenes 0..79\n",
            "Batch done in 168.5s | 80/1597 scenes | 0.47 scenes/s\n",
            "Batch 2/20 | scenes 80..159\n",
            "Batch done in 142.5s | 160/1597 scenes | 0.51 scenes/s\n",
            "Batch 3/20 | scenes 160..239\n",
            "Batch done in 187.7s | 240/1597 scenes | 0.48 scenes/s\n",
            "Batch 4/20 | scenes 240..319\n",
            "Batch done in 253.5s | 320/1597 scenes | 0.43 scenes/s\n",
            "Batch 5/20 | scenes 320..399\n",
            "Batch done in 190.6s | 400/1597 scenes | 0.42 scenes/s\n",
            "Batch 6/20 | scenes 400..479\n",
            "Batch done in 107.8s | 480/1597 scenes | 0.46 scenes/s\n",
            "Batch 7/20 | scenes 480..559\n",
            "Batch done in 185.8s | 560/1597 scenes | 0.45 scenes/s\n",
            "Batch 8/20 | scenes 560..639\n",
            "Batch done in 153.6s | 640/1597 scenes | 0.46 scenes/s\n",
            "Batch 9/20 | scenes 640..719\n",
            "Batch done in 183.9s | 720/1597 scenes | 0.46 scenes/s\n",
            "Batch 10/20 | scenes 720..799\n",
            "Batch done in 144.8s | 800/1597 scenes | 0.47 scenes/s\n",
            "Batch 11/20 | scenes 800..879\n",
            "Batch done in 180.0s | 880/1597 scenes | 0.46 scenes/s\n",
            "Batch 12/20 | scenes 880..959\n",
            "Batch done in 77.5s | 960/1597 scenes | 0.49 scenes/s\n",
            "Batch 13/20 | scenes 960..1039\n",
            "Batch done in 175.8s | 1040/1597 scenes | 0.48 scenes/s\n",
            "Batch 14/20 | scenes 1040..1119\n",
            "Batch done in 123.9s | 1120/1597 scenes | 0.49 scenes/s\n",
            "Batch 15/20 | scenes 1120..1199\n",
            "Batch done in 206.1s | 1200/1597 scenes | 0.48 scenes/s\n",
            "Batch 16/20 | scenes 1200..1279\n",
            "Batch done in 205.7s | 1280/1597 scenes | 0.48 scenes/s\n",
            "Batch 17/20 | scenes 1280..1359\n",
            "Batch done in 1070.0s | 1360/1597 scenes | 0.36 scenes/s\n",
            "Batch 18/20 | scenes 1360..1439\n",
            "Batch done in 194.3s | 1440/1597 scenes | 0.36 scenes/s\n",
            "Batch 19/20 | scenes 1440..1519\n",
            "Batch done in 167.9s | 1520/1597 scenes | 0.37 scenes/s\n",
            "Batch 20/20 | scenes 1520..1596\n",
            "Batch done in 138.2s | 1597/1597 scenes | 0.37 scenes/s\n",
            "Wrote ./outputs/STS3E4_structured_products.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "dict_keys(['video_id', 'scenes'])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# GPT request with strict JSON schema (faster + logged)\n",
        "from openai import OpenAI\n",
        "from pathlib import Path\n",
        "import time\n",
        "import math\n",
        "\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\n",
        "        f\"Missing OPENAI_API_KEY. Looked in {dotenv_path} (exists={os.path.exists(dotenv_path)})\"\n",
        "    )\n",
        "\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "MODEL = \"gpt-5\"\n",
        "BATCH_SIZE = 80\n",
        "MAX_CHARS = 400\n",
        "MAX_SCENES = None  # set to an int for quick runs\n",
        "\n",
        "schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"additionalProperties\": False,\n",
        "    \"properties\": {\n",
        "        \"video_id\": {\"type\": \"string\"},\n",
        "        \"scenes\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"additionalProperties\": False,\n",
        "                \"properties\": {\n",
        "                    \"scene_id\": {\"type\": \"string\"},\n",
        "                    \"timestamp_range\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"items\": {\"type\": \"number\"},\n",
        "                        \"minItems\": 2,\n",
        "                        \"maxItems\": 2,\n",
        "                    },\n",
        "                    \"product_mentions\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"object\",\n",
        "                            \"additionalProperties\": False,\n",
        "                            \"properties\": {\n",
        "                                \"product_name\": {\"type\": \"string\"},\n",
        "                                \"brand\": {\"type\": \"string\"},\n",
        "                                \"category\": {\"type\": \"string\"},\n",
        "                                \"confidence\": {\"type\": \"number\"},\n",
        "                                \"evidence\": {\n",
        "                                    \"type\": \"object\",\n",
        "                                    \"additionalProperties\": False,\n",
        "                                    \"properties\": {\n",
        "                                        \"visual\": {\"type\": \"string\"},\n",
        "                                        \"dialogue\": {\"type\": \"string\"},\n",
        "                                    },\n",
        "                                    \"required\": [\"visual\", \"dialogue\"],\n",
        "                                },\n",
        "                            },\n",
        "                            \"required\": [\n",
        "                                \"product_name\",\n",
        "                                \"brand\",\n",
        "                                \"category\",\n",
        "                                \"confidence\",\n",
        "                                \"evidence\",\n",
        "                            ],\n",
        "                        },\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"scene_id\", \"timestamp_range\", \"product_mentions\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"video_id\", \"scenes\"],\n",
        "}\n",
        "\n",
        "schema_compact = json.dumps(schema, separators=(\",\", \":\"))\n",
        "\n",
        "\n",
        "def get_scene_time_range(scene):\n",
        "    start = scene.get(\"start\") or scene.get(\"start_time\") or scene.get(\"start_sec\")\n",
        "    end = scene.get(\"end\") or scene.get(\"end_time\") or scene.get(\"end_sec\")\n",
        "    if start is None or end is None:\n",
        "        ts = scene.get(\"timestamp_range\") or scene.get(\"time_range\")\n",
        "        if isinstance(ts, list) and len(ts) == 2:\n",
        "            start, end = ts\n",
        "    return start, end\n",
        "\n",
        "\n",
        "def format_scene(scene):\n",
        "    start, end = get_scene_time_range(scene)\n",
        "    desc = scene.get(\"description\") or scene.get(\"caption\") or scene.get(\"summary\") or \"\"\n",
        "    return {\n",
        "        \"scene_id\": str(scene.get(\"scene_id\") or scene.get(\"id\") or \"\"),\n",
        "        \"timestamp_range\": [float(start or 0), float(end or 0)],\n",
        "        \"scene_description\": desc,\n",
        "    }\n",
        "\n",
        "\n",
        "def transcript_overlap_text(start, end, transcript_items, max_chars=MAX_CHARS):\n",
        "    if start is None or end is None:\n",
        "        return \"\"\n",
        "    chunks = []\n",
        "    length = 0\n",
        "    for item in transcript_items or []:\n",
        "        t_start = item.get(\"start\") or item.get(\"start_time\") or item.get(\"start_sec\")\n",
        "        t_end = item.get(\"end\") or item.get(\"end_time\") or item.get(\"end_sec\")\n",
        "        text = item.get(\"text\") or item.get(\"word\") or \"\"\n",
        "        if t_start is None or t_end is None:\n",
        "            continue\n",
        "        if t_end >= start and t_start <= end and text:\n",
        "            chunks.append(text)\n",
        "            length += len(text) + 1\n",
        "            if length >= max_chars:\n",
        "                break\n",
        "    joined = \" \".join(chunks)\n",
        "    return joined[:max_chars]\n",
        "\n",
        "\n",
        "# Shrink the payload to just what the model needs\n",
        "scenes_items = scenes if isinstance(scenes, list) else scenes.get(\"scenes\", [])\n",
        "transcript_items = transcript if isinstance(transcript, list) else transcript.get(\"segments\", [])\n",
        "\n",
        "if MAX_SCENES:\n",
        "    scenes_items = scenes_items[:MAX_SCENES]\n",
        "\n",
        "batch_size = BATCH_SIZE\n",
        "all_scene_results = []\n",
        "\n",
        "start_time = time.time()\n",
        "total_batches = math.ceil(len(scenes_items) / batch_size) if scenes_items else 0\n",
        "\n",
        "for batch_index in range(total_batches):\n",
        "    i = batch_index * batch_size\n",
        "    batch = scenes_items[i : i + batch_size]\n",
        "    compact_batch = []\n",
        "    for scene in batch:\n",
        "        start, end = get_scene_time_range(scene)\n",
        "        compact_batch.append(\n",
        "            {\n",
        "                **format_scene(scene),\n",
        "                \"dialogue_excerpt\": transcript_overlap_text(start, end, transcript_items),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    batch_payload = {\n",
        "        \"video_id\": video.id,\n",
        "        \"scenes\": compact_batch,\n",
        "    }\n",
        "\n",
        "    print(f\"Batch {batch_index + 1}/{total_batches} | scenes {i}..{i + len(batch) - 1}\")\n",
        "    batch_start = time.time()\n",
        "\n",
        "    response = client.responses.create(\n",
        "        model=MODEL,\n",
        "        input=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"Extract product mentions per scene using the scene_description and dialogue_excerpt. \"\n",
        "                    \"Return ONLY valid JSON that matches this JSON Schema and nothing else: \"\n",
        "                    + schema_compact\n",
        "                ),\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": json.dumps(batch_payload, separators=(\",\", \":\")),\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    content = response.output_text\n",
        "    structured = json.loads(content)\n",
        "    all_scene_results.extend(structured.get(\"scenes\", []))\n",
        "\n",
        "    batch_elapsed = time.time() - batch_start\n",
        "    done_scenes = min((batch_index + 1) * batch_size, len(scenes_items))\n",
        "    total_elapsed = time.time() - start_time\n",
        "    rate = done_scenes / max(total_elapsed, 1)\n",
        "    print(f\"Batch done in {batch_elapsed:.1f}s | {done_scenes}/{len(scenes_items)} scenes | {rate:.2f} scenes/s\")\n",
        "\n",
        "output_prefix = Path(video_path).stem\n",
        "final_structured = {\n",
        "    \"video_id\": video.id,\n",
        "    \"scenes\": all_scene_results,\n",
        "}\n",
        "\n",
        "with open(f\"./outputs/{output_prefix}_structured_products.json\", \"w\") as f:\n",
        "    json.dump(final_structured, f, indent=2)\n",
        "\n",
        "print(f\"Wrote ./outputs/{output_prefix}_structured_products.json\")\n",
        "final_structured.keys()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
